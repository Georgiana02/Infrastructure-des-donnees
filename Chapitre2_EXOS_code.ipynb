{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "110cc7f4",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "**1**: Lorem Ipsum is just a random txt that devs use as a placeholder for multiple things (especially web developping) when you don't have the real text and just want to test your functionnality. Put a [Lorem Ipsum](https://www.lipsum.com/) of 3 paragraphs in a txt file using python, each paragraph delimited by two new line.\n",
    "\n",
    "**2**: Update the txt file by removing the first paragraph.\n",
    "\n",
    "**3**: Create a dict from the paper of [lecun et al.](https://www.researchgate.net/publication/277411157_Deep_Learning) and [goodfellow et al.](https://arxiv.org/abs/1406.2661) with authors, title, affiliations.\n",
    "\n",
    "**4**: Save the previously created dict in the JSON format and load it back.\n",
    "\n",
    "**5**: Save the previously created dict in the pickle format. Try to open manually (i.e with a text editor), is it human readable ?\n",
    "\n",
    "**6**: Parse the xml_file2 in the same way as in the lecture. put infos in a dict and save it in a json file.\n",
    "\n",
    "**7**: Download an image of your choice and save it in either jpg or png.\n",
    "\n",
    "**8**: From the data/Chap2/data_world.json file, create a set of publisher type.\n",
    "\n",
    "**9**: From the data/Chap2/data_world.json file, delete the key of your choice and save the new dict as data_world_cleaned.json.\n",
    "\n",
    "**10**: From the data/Chap2/data_world.json file, create the co-occurence matrix between \"accessLevel\" and \"accrualPeriodicity\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504c64e1",
   "metadata": {},
   "source": [
    "## 1) Lorem Ipsum is just a random txt that devs use as a placeholder for multiple things (especially web developping) when you don't have the real text and just want to test your functionnality. Put a Lorem Ipsum of 3 paragraphs in a txt file using python, each paragraph delimited by two new line.¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d79b0f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labore est eius voluptatem sit ut sed ipsum. Est aliquam est quaerat adipisci. Modi magnam numquam neque labore. Quiquia eius dolore quaerat neque. Eius quaerat dolor sit eius ipsum. Quaerat adipisci non quaerat porro quiquia. Numquam quisquam amet amet consectetur est. Porro consectetur neque tempora.\n",
      "\n",
      "Labore tempora dolor numquam. Etincidunt dolor dolorem adipisci ut quaerat dolorem. Dolore sed aliquam etincidunt sit dolorem. Aliquam modi consectetur tempora amet ut quiquia velit. Labore porro consectetur labore velit dolor. Dolore voluptatem sed quaerat. Quaerat ipsum sed neque sed ipsum etincidunt ipsum. Magnam est dolor sit aliquam.\n",
      "\n",
      "Est quiquia non sed non dolore tempora eius. Sed numquam adipisci dolore. Sit dolor tempora consectetur ut etincidunt non quiquia. Sed adipisci magnam modi labore sit est. Ut ipsum modi magnam porro ipsum voluptatem. Magnam dolorem modi etincidunt adipisci voluptatem. Non amet quaerat aliquam dolor ut.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lorem\n",
    "\n",
    "def generate_lorem_ipsum_paragraphs(num_paragraphs=3):\n",
    "    paragraphs = [lorem.paragraph() for _ in range(num_paragraphs)]\n",
    "    return paragraphs\n",
    "\n",
    "paragraphs = generate_lorem_ipsum_paragraphs(3)\n",
    "for paragraph in paragraphs:\n",
    "    print(paragraph)\n",
    "    print()\n",
    "def save_paragraphs_to_file(paragraphs, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for paragraph in paragraphs:\n",
    "            file.write(paragraph + '\\n\\n')\n",
    "\n",
    "paragraphs = generate_lorem_ipsum_paragraphs(3)\n",
    "filename = \"lorem_paragraphs.txt\"\n",
    "save_paragraphs_to_file(paragraphs, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca9b4ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eius dolorem labore sed ut dolore dolorem quisquam. Magnam dolore aliquam quaerat labore dolore. Labore porro est aliquam labore ipsum modi. Etincidunt eius tempora est quisquam tempora eius labore. Quiquia etincidunt aliquam numquam non modi. Amet voluptatem consectetur non velit velit voluptatem. Modi sed numquam eius consectetur est. Ipsum tempora quisquam numquam eius. Velit velit sit tempora dolorem.\n",
      "\n",
      "Aliquam neque quisquam modi quisquam sed dolore. Aliquam sit porro labore ut. Tempora voluptatem magnam dolore est. Neque neque consectetur sed neque. Velit quaerat numquam labore est amet dolor sit.\n",
      "\n",
      "Etincidunt velit adipisci amet dolore aliquam. Sed non amet velit. Quaerat eius magnam aliquam. Tempora dolor adipisci porro non dolor amet neque. Velit numquam sit numquam sed.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"lorem_paragraphs.txt\", 'r') as file:\n",
    "        content = file.read()\n",
    "        print(content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4786f4bd",
   "metadata": {},
   "source": [
    "## 2) Update the txt file by removing the first paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52648506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aliquam neque quisquam modi quisquam sed dolore. Aliquam sit porro labore ut. Tempora voluptatem magnam dolore est. Neque neque consectetur sed neque. Velit quaerat numquam labore est amet dolor sit.\n",
      "\n",
      "Etincidunt velit adipisci amet dolore aliquam. Sed non amet velit. Quaerat eius magnam aliquam. Tempora dolor adipisci porro non dolor amet neque. Velit numquam sit numquam sed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filename = \"lorem_paragraphs.txt\"\n",
    "\n",
    "with open(filename, 'r') as file:\n",
    "    # Lire les lignes du fichier\n",
    "    lines = file.readlines()\n",
    "\n",
    "empty_line_index = lines.index('\\n')\n",
    "\n",
    "for line in lines[empty_line_index+1:]:\n",
    "    print(line.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99c1b1d",
   "metadata": {},
   "source": [
    "## 3) Create a dict from the paper of lecun et al. and goodfellow et al. with authors, title, affiliations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ff882fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title of LeCun's paper: Gradient-based learning applied to document recognition\n",
      "Authors of Goodfellow's paper: ['Ian Goodfellow', 'Yoshua Bengio', 'Aaron Courville']\n"
     ]
    }
   ],
   "source": [
    "papers_dict = {\n",
    "    \"LeCun_et_al\": {\n",
    "        \"title\": \"Gradient-based learning applied to document recognition\",\n",
    "        \"authors\": [\"Anna LeCun\", \"Bernard Boser\", \"Jeanne S. Denker\", \"Donnie Henderson\"],\n",
    "        \"affiliations\": [\"Harvard University, University of Strasbourg, Columbia University, Yale University\"]\n",
    "    },\n",
    "    \"Goodfellow_et_al\": {\n",
    "        \"title\": \"Generative Adversarial Nets\",\n",
    "        \"authors\": [\"Ian Goodfellow\", \"Yoshua Bengio\", \"Aaron Courville\"],\n",
    "        \"affiliations\": [\"Montreal Institute for Learning Algorithms (MILA), University of Montreal, McGill University\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Title of LeCun's paper:\", papers_dict[\"LeCun_et_al\"][\"title\"])\n",
    "print(\"Authors of Goodfellow's paper:\", papers_dict[\"Goodfellow_et_al\"][\"authors\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ef240d",
   "metadata": {},
   "source": [
    "## 4) Save the previously created dict in the JSON format and load it back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "908e9983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'LeCun_et_al': {'title': 'Gradient-based learning applied to document recognition', 'authors': ['Anna LeCun', 'Bernard Boser', 'Jeanne S. Denker', 'Donnie Henderson'], 'affiliations': ['Harvard University, University of Strasbourg, Columbia University, Yale University']}, 'Goodfellow_et_al': {'title': 'Generative Adversarial Nets', 'authors': ['Ian Goodfellow', 'Yoshua Bengio', 'Aaron Courville'], 'affiliations': ['Montreal Institute for Learning Algorithms (MILA), University of Montreal, McGill University']}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"papers.json\", \"w\") as json_file:\n",
    "    json.dump(papers_dict, json_file, indent=4)\n",
    "\n",
    "with open(\"papers.json\", \"r\") as json_file:\n",
    "    loaded_papers_dict = json.load(json_file)\n",
    "\n",
    "print(loaded_papers_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7867c119",
   "metadata": {},
   "source": [
    "## 5) Save the previously created dict in the pickle format. Try to open manually (i.e with a text editor), is it human readable ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a098296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "€\u0004•å\u0001\u0000\u0000\u0000\u0000\u0000\u0000}”(Œ\u000b",
      "LeCun_et_al”}”(Œ\u0005title”Œ7Gradient-based learning applied to document recognition”Œ\u0007authors”]”(Œ\n",
      "Anna LeCun”Œ\n",
      "Bernard Boser”Œ\u0010Jeanne S. Denker”Œ\u0010Donnie Henderson”eŒ\f",
      "affiliations”]”ŒRHarvard University, University of Strasbourg, Columbia University, Yale University”auŒ\u0010Goodfellow_et_al”}”(h\u0003Œ\u001bGenerative Adversarial Nets”h\u0005]”(Œ\u000eIan Goodfellow”Œ\n",
      "Yoshua Bengio”Œ\u000fAaron Courville”eh\u000b",
      "]”Œ\\Montreal Institute for Learning Algorithms (MILA), University of Montreal, McGill University”auu.\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "with open(\"papers.pickle\", \"wb\") as pickle_file:\n",
    "    pickle.dump(papers_dict, pickle_file)\n",
    "with open('papers.pickle', 'r', errors='ignore') as file:\n",
    "    content = file.read()\n",
    "\n",
    "print(content)\n",
    "\n",
    "#Non, on ne peut pas le lire avec un text editor parce que c'est en format binaire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e410df",
   "metadata": {},
   "source": [
    "## 6) Parse the xml_file2 in the same way as in the lecture. put infos in a dict and save it in a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0152a664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<note>\n",
      "  <date>2015-09-01</date>\n",
      "  <hour>08:30</hour>\n",
      "  <to>Tove</to>\n",
      "  <from>Jani</from>\n",
      "  <body>Don't forget me this weekend!</body>\n",
      "</note>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import lxml.etree\n",
    "\n",
    "xml_file = \"C:/Users/Georgiana/Downloads/xml_file2.nxml\"\n",
    "root = lxml.etree.parse(xml_file)\n",
    "print(lxml.etree.tostring(root, encoding=\"unicode\", pretty_print=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b483cce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': '2015-09-01',\n",
       " 'hour': '08:30',\n",
       " 'to': 'Tove',\n",
       " 'from': 'Jani',\n",
       " 'body': \"Don't forget me this weekend!\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import lxml.etree\n",
    "xml_file = \"C:/Users/Georgiana/Downloads/xml_file2.nxml\"\n",
    "\n",
    "\n",
    "root = lxml.etree.parse(xml_file)\n",
    "\n",
    "xml_data = {}\n",
    "for element in root.iter():\n",
    "    if element.tag == \"note\":\n",
    "        xml_data[\"date\"] = element.find(\"date\").text\n",
    "        xml_data[\"hour\"] = element.find(\"hour\").text\n",
    "        xml_data[\"to\"] = element.find(\"to\").text\n",
    "        xml_data[\"from\"] = element.find(\"from\").text\n",
    "        xml_data[\"body\"] = element.find(\"body\").text\n",
    "json_file = \"xml_data.json\"\n",
    "with open(json_file, \"w\") as file:\n",
    "    json.dump(xml_data, file)\n",
    "\n",
    "xml_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48b57608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xmltodict in c:\\users\\georgiana\\anaconda3\\lib\\site-packages (0.13.0)\n",
      "<note>\n",
      "  <date>2015-09-01</date>\n",
      "  <hour>08:30</hour>\n",
      "  <to>Tove</to>\n",
      "  <from>Jani</from>\n",
      "  <body>Don't forget me this weekend!</body>\n",
      "</note>\n",
      "\n",
      "{'note': {'date': '2015-09-01', 'hour': '08:30', 'to': 'Tove', 'from': 'Jani', 'body': \"Don't forget me this weekend!\"}}\n"
     ]
    }
   ],
   "source": [
    "import lxml.etree\n",
    "!pip install xmltodict\n",
    "import xmltodict\n",
    "import json\n",
    "\n",
    "xml_file = \"C:/Users/Georgiana/Downloads/xml_file2.nxml\"\n",
    "root = lxml.etree.parse(xml_file)\n",
    "print(lxml.etree.tostring(root, encoding=\"unicode\", pretty_print=True)) \n",
    "with open(xml_file, \"rb\") as f:\n",
    "    xml_dict = xmltodict.parse(f)\n",
    "print(xml_dict)\n",
    "\n",
    "json_data = json.dumps(xml_dict, indent=4)\n",
    "\n",
    "with open(\"xml_data.json\", \"w\") as json_file:\n",
    "    json_file.write(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccab04c8",
   "metadata": {},
   "source": [
    "## 7) Download an image of your choice and save it in either jpg or png."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04dc7dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=1600x1200 at 0x1AA28057110>\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "im = Image.open(\"C:/Users/Georgiana/Downloads/fleur.jpg\")\n",
    "im.save(\"C:/Users/Georgiana/Downloads/fleur.jpg\", \"png\")\n",
    "print(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cee2f74",
   "metadata": {},
   "source": [
    "## 8) From the data/Chap2/data_world.json file, create a set of publisher type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c89804c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'org:Organization'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"C:/Users/Georgiana/Downloads/data_world.json\") as file:\n",
    "    json_data = json.load(file)\n",
    "    publisher_types = set()\n",
    "\n",
    "    for item in json_data:\n",
    "        publisher = item.get('publisher')\n",
    "\n",
    "        if publisher and '@type' in publisher:\n",
    "            publisher_types.add(publisher['@type'])\n",
    "\n",
    "print(publisher_types)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f100ed",
   "metadata": {},
   "source": [
    "## 9) From the data/Chap2/data_world.json file, delete the key of your choice and save the new dict as data_world_cleaned.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3c3f8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key 'spatial' deleted and data saved to data_world_cleaned.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"C:/Users/Georgiana/Downloads/data_world.json\") as file:\n",
    "    data_world = json.load(file)\n",
    "\n",
    "key_to_delete = \"spatial\"\n",
    "if key_to_delete in data_world:\n",
    "    del data_world[key_to_delete]\n",
    "\n",
    "with open('data_world_cleaned.json', 'w') as file:\n",
    "    json.dump(data_world, file, indent=4)\n",
    "\n",
    "print(\"Key '{}' deleted and data saved to data_world_cleaned.json\".format(key_to_delete))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1925d1",
   "metadata": {},
   "source": [
    "## 10) From the data/Chap2/data_world.json file, create the co-occurence matrix between \"accessLevel\" and \"accrualPeriodicity\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43c7548d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-occurrence Matrix:\n",
      "Access Level: public\n",
      "\tAccrual Periodicity: irregular - Count: 4961\n",
      "\tAccrual Periodicity: R/P1D - Count: 5\n",
      "\tAccrual Periodicity: R/P1M - Count: 3\n",
      "\tAccrual Periodicity: R/PT1S - Count: 1\n",
      "\tAccrual Periodicity: R/P3M - Count: 1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open(\"C:/Users/Georgiana/Downloads/data_world.json\") as file:\n",
    "    data_world = json.load(file)\n",
    "\n",
    "\n",
    "co_occurrence_matrix = {}\n",
    "\n",
    "\n",
    "for item in data_world:\n",
    "    access_level = item.get('accessLevel')\n",
    "    accrual_periodicity = item.get('accrualPeriodicity')\n",
    "\n",
    "    \n",
    "    if access_level and accrual_periodicity:\n",
    "        if access_level not in co_occurrence_matrix:\n",
    "            co_occurrence_matrix[access_level] = {}\n",
    "        if accrual_periodicity not in co_occurrence_matrix[access_level]:\n",
    "            co_occurrence_matrix[access_level][accrual_periodicity] = 0\n",
    "        co_occurrence_matrix[access_level][accrual_periodicity] += 1\n",
    "\n",
    "\n",
    "print(\"Co-occurrence Matrix:\")\n",
    "for access_level, accrual_counts in co_occurrence_matrix.items():\n",
    "    print(f\"Access Level: {access_level}\")\n",
    "    for accrual_periodicity, count in accrual_counts.items():\n",
    "        print(f\"\\tAccrual Periodicity: {accrual_periodicity} - Count: {count}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b12bc9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
